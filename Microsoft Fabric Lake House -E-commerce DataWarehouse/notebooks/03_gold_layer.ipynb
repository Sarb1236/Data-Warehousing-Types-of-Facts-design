{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold Layer - Star Schema Creation\n",
        "\n",
        "This notebook creates the final star schema with fact and dimension tables.\n",
        "\n",
        "## Features:\n",
        "- **Star Schema**: Dimensions and facts with proper joins\n",
        "- **SCD2 Support**: Uses only current dimension records\n",
        "- **Config-driven**: Uses `config/ingestion_config.json`\n",
        "- **Control tables**: Audit log, watermark, error records\n",
        "- **Multiple fact types**: Transactional, accumulating, snapshot\n",
        "\n",
        "## Fact Types:\n",
        "- **Transactional Facts**: Order transactions\n",
        "- **Accumulating Facts**: Customer order summaries\n",
        "- **Snapshot Facts**: Inventory snapshots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import os, json, uuid, datetime\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Gold Layer - Star Schema\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def now(): return datetime.datetime.now().isoformat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load config\n",
        "with open('../config/ingestion_config.json') as f:\n",
        "    config = json.load(f)\n",
        "gold_cfg = config['gold_layer']\n",
        "control_cfg = config['control_tables']\n",
        "\n",
        "SILVER_DATA_PATH = gold_cfg['silver_data_path']\n",
        "GOLD_DATA_PATH = gold_cfg['gold_data_path']\n",
        "os.makedirs(GOLD_DATA_PATH, exist_ok=True)\n",
        "\n",
        "def log_audit(table, layer, status, row_count, error=None):\n",
        "    run_id = str(uuid.uuid4())\n",
        "    try:\n",
        "        with open(control_cfg['audit_log'], 'a') as f:\n",
        "            f.write(f'{run_id},{table},{layer},{now()},,{status},{row_count},{error or \"\"}\\n')\n",
        "        return run_id\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log audit: {e}\")\n",
        "        return None\n",
        "\n",
        "def log_error(run_id, table, layer, error_type, error_message, record):\n",
        "    try:\n",
        "        with open(control_cfg['error_records'], 'a') as f:\n",
        "            f.write(f'{run_id},{table},{layer},{error_type},{error_message},{json.dumps(record)},{now()}\\n')\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log error: {e}\")\n",
        "\n",
        "def update_watermark(table, layer, key, date):\n",
        "    try:\n",
        "        with open(control_cfg['watermark'], 'a') as f:\n",
        "            f.write(f'{table},{layer},{key},{date}\\n')\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not update watermark: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dimension tables (only current records)\n",
        "print(\"üîÑ Creating dimension tables...\")\n",
        "\n",
        "for dim_name, dim_cfg in gold_cfg['dimensions'].items():\n",
        "    try:\n",
        "        print(f\"üìä Processing dimension: {dim_name}\")\n",
        "        \n",
        "        # Read from silver (SCD2)\n",
        "        df_silver = spark.read.format('delta').load(f'{SILVER_DATA_PATH}/{dim_name}')\n",
        "        \n",
        "        # Filter only current records\n",
        "        df_current = df_silver.filter(col('is_current') == True)\n",
        "        \n",
        "        # Select required columns\n",
        "        columns = dim_cfg['columns']\n",
        "        df_dim = df_current.select(columns)\n",
        "        \n",
        "        # Write to gold\n",
        "        gold_dim_path = f'{GOLD_DATA_PATH}/{dim_name}'\n",
        "        df_dim.write.mode('overwrite').format('delta').save(gold_dim_path)\n",
        "        \n",
        "        print(f\"‚úÖ {dim_name}: {df_dim.count()} rows\")\n",
        "        log_audit(dim_name, 'gold', 'SUCCESS', df_dim.count())\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"‚ùå Error processing {dim_name}: {error_msg}\")\n",
        "        log_audit(dim_name, 'gold', 'FAIL', 0, error_msg)\n",
        "\n",
        "print(\"‚úÖ Dimensions created successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fact tables\n",
        "print(\"\\nüîÑ Creating fact tables...\")\n",
        "\n",
        "for fact_name, fact_cfg in gold_cfg['facts'].items():\n",
        "    try:\n",
        "        print(f\"üìä Processing fact: {fact_name}\")\n",
        "        \n",
        "        # Read source table from silver\n",
        "        source_table = fact_cfg['source_table']\n",
        "        df_source = spark.read.format('delta').load(f'{SILVER_DATA_PATH}/{source_table}')\n",
        "        \n",
        "        # Apply joins if specified\n",
        "        if 'joins' in fact_cfg:\n",
        "            for join_config in fact_cfg['joins']:\n",
        "                dim_table = join_config['table']\n",
        "                join_key = join_config['on']\n",
        "                join_type = join_config['type']\n",
        "                \n",
        "                # Read dimension\n",
        "                df_dim = spark.read.format('delta').load(f'{GOLD_DATA_PATH}/{dim_table}')\n",
        "                \n",
        "                # Perform join\n",
        "                df_source = df_source.join(df_dim, join_key, join_type)\n",
        "                print(f\"üîó Joined with {dim_table}\")\n",
        "        \n",
        "        # Apply aggregations if specified\n",
        "        if 'aggregations' in fact_cfg:\n",
        "            agg_config = fact_cfg['aggregations']\n",
        "            group_cols = agg_config['group_by']\n",
        "            metrics = agg_config['metrics']\n",
        "            \n",
        "            # Build aggregation expressions\n",
        "            agg_exprs = []\n",
        "            for metric_name, metric_expr in metrics.items():\n",
        "                agg_exprs.append(f\"{metric_expr} as {metric_name}\")\n",
        "            \n",
        "            # Group by and aggregate\n",
        "            df_source = df_source.groupBy(group_cols).agg(*[expr(agg_expr) for agg_expr in agg_exprs])\n",
        "            print(f\"üìà Applied aggregations\")\n",
        "        \n",
        "        # Select final columns\n",
        "        if 'columns' in fact_cfg:\n",
        "            columns = fact_cfg['columns']\n",
        "            df_fact = df_source.select(columns)\n",
        "        else:\n",
        "            df_fact = df_source\n",
        "        \n",
        "        # Write to gold\n",
        "        gold_fact_path = f'{GOLD_DATA_PATH}/{fact_name}'\n",
        "        df_fact.write.mode('overwrite').format('delta').save(gold_fact_path)\n",
        "        \n",
        "        print(f\"‚úÖ {fact_name}: {df_fact.count()} rows\")\n",
        "        log_audit(fact_name, 'gold', 'SUCCESS', df_fact.count())\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"‚ùå Error processing {fact_name}: {error_msg}\")\n",
        "        log_audit(fact_name, 'gold', 'FAIL', 0, error_msg)\n",
        "\n",
        "print(\"‚úÖ Facts created successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create analytical views\n",
        "print(\"\\nüîÑ Creating analytical views...\")\n",
        "\n",
        "if 'analytical_views' in gold_cfg:\n",
        "    for view_name, view_config in gold_cfg['analytical_views'].items():\n",
        "        try:\n",
        "            print(f\"üìä Creating view: {view_name}\")\n",
        "            \n",
        "            # Execute the query\n",
        "            query = view_config['query']\n",
        "            df_view = spark.sql(query)\n",
        "            \n",
        "            # Write to gold\n",
        "            gold_view_path = f'{GOLD_DATA_PATH}/{view_name}'\n",
        "            df_view.write.mode('overwrite').format('delta').save(gold_view_path)\n",
        "            \n",
        "            print(f\"‚úÖ {view_name}: {df_view.count()} rows\")\n",
        "            log_audit(view_name, 'gold', 'SUCCESS', df_view.count())\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"‚ùå Error creating {view_name}: {error_msg}\")\n",
        "            log_audit(view_name, 'gold', 'FAIL', 0, error_msg)\n",
        "\n",
        "print(\"\\nüéâ Gold layer star schema creation completed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
