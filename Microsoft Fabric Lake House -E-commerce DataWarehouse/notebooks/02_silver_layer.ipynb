{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Silver Layer - SCD2 Data Transformation\n",
        "\n",
        "This notebook transforms bronze layer data into SCD2-compliant silver layer data.\n",
        "\n",
        "## Features:\n",
        "- **SCD2 for all dimensions**: Surrogate keys, effective dates, is_current\n",
        "- **Config-driven**: Uses `config/ingestion_config.json`\n",
        "- **Control tables**: Audit log, watermark, error records\n",
        "- **Idempotent, production-ready**\n",
        "\n",
        "## SCD2 Implementation:\n",
        "- Business keys for change detection\n",
        "- Effective start/end dates\n",
        "- is_current flag\n",
        "- Surrogate keys for dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import os, json, uuid, datetime\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Silver Layer - SCD2\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def now(): return datetime.datetime.now().isoformat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load config\n",
        "with open('../config/ingestion_config.json') as f:\n",
        "    config = json.load(f)\n",
        "silver_cfg = config['silver_layer']\n",
        "gold_cfg = config['gold_layer']\n",
        "control_cfg = config['control_tables']\n",
        "\n",
        "BRONZE_DATA_PATH = silver_cfg['bronze_data_path']\n",
        "SILVER_DATA_PATH = silver_cfg['silver_data_path']\n",
        "os.makedirs(SILVER_DATA_PATH, exist_ok=True)\n",
        "\n",
        "def log_audit(table, layer, status, row_count, error=None):\n",
        "    run_id = str(uuid.uuid4())\n",
        "    try:\n",
        "        with open(control_cfg['audit_log'], 'a') as f:\n",
        "            f.write(f'{run_id},{table},{layer},{now()},,{status},{row_count},{error or \"\"}\\n')\n",
        "        return run_id\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log audit: {e}\")\n",
        "        return None\n",
        "\n",
        "def log_error(run_id, table, layer, error_type, error_message, record):\n",
        "    try:\n",
        "        with open(control_cfg['error_records'], 'a') as f:\n",
        "            f.write(f'{run_id},{table},{layer},{error_type},{error_message},{json.dumps(record)},{now()}\\n')\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log error: {e}\")\n",
        "\n",
        "def update_watermark(table, layer, key, date):\n",
        "    try:\n",
        "        with open(control_cfg['watermark'], 'a') as f:\n",
        "            f.write(f'{table},{layer},{key},{date}\\n')\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not update watermark: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SCD2 Upsert Function\n",
        "def scd2_upsert(df_new, dim_name, dim_cfg):\n",
        "    \"\"\"Perform SCD2 upsert for dimension tables\"\"\"\n",
        "    print(f\"üîÑ Processing SCD2 for {dim_name}...\")\n",
        "    \n",
        "    # Load existing dimension if exists\n",
        "    dim_path = f'{SILVER_DATA_PATH}/{dim_name}'\n",
        "    try:\n",
        "        df_existing = spark.read.format('delta').load(dim_path)\n",
        "        print(f\"üìñ Loaded existing dimension: {df_existing.count()} rows\")\n",
        "    except:\n",
        "        df_existing = spark.createDataFrame([], df_new.schema)\n",
        "        print(f\"üìù Creating new dimension\")\n",
        "    \n",
        "    # SCD2 configuration\n",
        "    business_key = dim_cfg['business_key']\n",
        "    scd2_fields = dim_cfg['scd2_fields']\n",
        "    surrogate_key_col = f'{dim_name}_sk'\n",
        "    \n",
        "    # Add SCD2 columns to new data\n",
        "    df_new = df_new.withColumn('effective_start_date', current_date()) \\\n",
        "                   .withColumn('effective_end_date', lit('9999-12-31')) \\\n",
        "                   .withColumn('is_current', lit(True)) \\\n",
        "                   .withColumn(surrogate_key_col, monotonically_increasing_id())\n",
        "    \n",
        "    if df_existing.count() == 0:\n",
        "        # First time load\n",
        "        result = df_new\n",
        "    else:\n",
        "        # Join on business key, find changes\n",
        "        join_cond = [df_new[business_key] == df_existing[business_key], df_existing['is_current'] == True]\n",
        "        df_joined = df_new.join(df_existing, join_cond, 'left_outer')\n",
        "        \n",
        "        # Detect changes in SCD2 fields\n",
        "        change_conditions = []\n",
        "        for field in scd2_fields:\n",
        "            if field in df_new.columns and field in df_existing.columns:\n",
        "                change_conditions.append(f\"df_new.{field} != df_existing.{field}\")\n",
        "        \n",
        "        if change_conditions:\n",
        "            change_expr = \" OR \".join(change_conditions)\n",
        "            changed = df_joined.filter(change_expr)\n",
        "            \n",
        "            # Expire old records\n",
        "            expired = df_existing.join(changed, business_key, 'inner') \\\n",
        "                .withColumn('effective_end_date', current_date()) \\\n",
        "                .withColumn('is_current', lit(False))\n",
        "            \n",
        "            # New records with new surrogate keys\n",
        "            new_records = changed.withColumn(surrogate_key_col, monotonically_increasing_id())\n",
        "            \n",
        "            # Union unchanged, expired, and new\n",
        "            unchanged = df_existing.join(changed, business_key, 'left_anti')\n",
        "            result = unchanged.unionByName(expired).unionByName(new_records)\n",
        "        else:\n",
        "            # No changes detected\n",
        "            result = df_existing\n",
        "    \n",
        "    # Write to silver\n",
        "    result.write.mode('overwrite').format('delta').save(dim_path)\n",
        "    print(f\"üíæ Written to silver: {dim_path} ({result.count()} rows)\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all SCD2 dimensions\n",
        "for dim_name, dim_cfg in gold_cfg['dimensions'].items():\n",
        "    print(f\"\\nüîÑ Processing {dim_name} (SCD2)...\")\n",
        "    try:\n",
        "        # Read from bronze\n",
        "        bronze_table = dim_cfg['source_table']\n",
        "        df_bronze = spark.read.format('delta').load(f'{BRONZE_DATA_PATH}/{bronze_table}')\n",
        "        \n",
        "        # Apply SCD2 transformation\n",
        "        df_silver = scd2_upsert(df_bronze, dim_name, dim_cfg)\n",
        "        \n",
        "        # Log success\n",
        "        log_audit(dim_name, 'silver', 'SUCCESS', df_silver.count())\n",
        "        update_watermark(dim_name, 'silver', None, now())\n",
        "        \n",
        "        print(f\"‚úÖ {dim_name} processed successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"‚ùå Error processing {dim_name}: {error_msg}\")\n",
        "        \n",
        "        # Log error\n",
        "        run_id = log_audit(dim_name, 'silver', 'FAIL', 0, error_msg)\n",
        "        log_error(run_id, dim_name, 'silver', 'SCD2', error_msg, {})\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nüéâ Silver layer SCD2 processing completed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
