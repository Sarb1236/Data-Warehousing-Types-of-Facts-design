{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bronze Layer - Configuration Driven Data Ingestion\n",
        "\n",
        "This notebook ingests raw CSV data into the bronze layer using configuration-driven approach.\n",
        "\n",
        "## Features:\n",
        "- **Config-driven**: Uses `config/ingestion_config.json`\n",
        "- **Data Quality Checks**: Null, duplicate, data type, business rules\n",
        "- **Error Handling**: Logs errors to control tables\n",
        "- **Audit Trail**: Tracks all operations\n",
        "\n",
        "## How to Add New Tables:\n",
        "1. Add CSV file to `data/raw/`\n",
        "2. Update `config/ingestion_config.json`\n",
        "3. Re-run this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "import datetime\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Bronze Layer - Config Driven\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def now(): return datetime.datetime.now().isoformat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "def load_config():\n",
        "    \"\"\"Load ingestion configuration from JSON file\"\"\"\n",
        "    config_path = \"../config/ingestion_config.json\"\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = json.load(f)\n",
        "        print(f\"\u2705 Configuration loaded from {config_path}\")\n",
        "        return config\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error loading config: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Load configuration\n",
        "config = load_config()\n",
        "if config is None:\n",
        "    raise Exception(\"Failed to load configuration\")\n",
        "\n",
        "# Extract paths from config\n",
        "bronze_config = config['bronze_layer']\n",
        "RAW_DATA_PATH = bronze_config['raw_data_path']\n",
        "BRONZE_DATA_PATH = bronze_config['bronze_data_path']\n",
        "control_config = config['control_tables']\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(BRONZE_DATA_PATH, exist_ok=True)\n",
        "print(f\"\ud83d\udcc1 Bronze data path: {BRONZE_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Control table functions\n",
        "def log_audit(table, layer, status, row_count, error=None):\n",
        "    \"\"\"Log audit information to control table\"\"\"\n",
        "    run_id = str(uuid.uuid4())\n",
        "    try:\n",
        "        with open(control_config['audit_log'], 'a') as f:\n",
        "            f.write(f'{run_id},{table},{layer},{now()},,{status},{row_count},{error or \"\"}\\n')\n",
        "        return run_id\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log audit: {e}\")\n",
        "        return None\n",
        "\n",
        "def log_error(run_id, table, layer, error_type, error_message, record):\n",
        "    \"\"\"Log error information to control table\"\"\"\n",
        "    try:\n",
        "        with open(control_config['error_records'], 'a') as f:\n",
        "            f.write(f'{run_id},{table},{layer},{error_type},{error_message},{json.dumps(record)},{now()}\\n')\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log error: {e}\")\n",
        "\n",
        "def update_watermark(table, layer, key, date):\n",
        "    \"\"\"Update watermark control table\"\"\"\n",
        "    try:\n",
        "        with open(control_config['watermark'], 'a') as f:\n",
        "            f.write(f'{table},{layer},{key},{date}\\n')\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not update watermark: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality validation functions\n",
        "def validate_null_checks(df, null_columns):\n",
        "    \"\"\"Validate null checks for specified columns\"\"\"\n",
        "    if not null_columns:\n",
        "        return df\n",
        "    \n",
        "    null_condition = \" AND \".join([f\"{col} IS NOT NULL\" for col in null_columns])\n",
        "    return df.filter(null_condition)\n",
        "\n",
        "def validate_duplicate_checks(df, duplicate_columns):\n",
        "    \"\"\"Remove duplicates based on specified columns\"\"\"\n",
        "    if not duplicate_columns:\n",
        "        return df\n",
        "    \n",
        "    return df.dropDuplicates(duplicate_columns)\n",
        "\n",
        "def validate_data_types(df, data_type_rules):\n",
        "    \"\"\"Cast columns to specified data types\"\"\"\n",
        "    if not data_type_rules:\n",
        "        return df\n",
        "    \n",
        "    for col, data_type in data_type_rules.items():\n",
        "        if col in df.columns:\n",
        "            if data_type == \"int\":\n",
        "                df = df.withColumn(col, col(col).cast(\"int\"))\n",
        "            elif data_type == \"decimal\":\n",
        "                df = df.withColumn(col, col(col).cast(\"decimal(10,2)\"))\n",
        "            elif data_type == \"string\":\n",
        "                df = df.withColumn(col, col(col).cast(\"string\"))\n",
        "    return df\n",
        "\n",
        "def validate_business_rules(df, business_rules):\n",
        "    \"\"\"Apply business rule validations\"\"\"\n",
        "    if not business_rules:\n",
        "        return df\n",
        "    \n",
        "    conditions = []\n",
        "    for col, rule in business_rules.items():\n",
        "        if col in df.columns:\n",
        "            conditions.append(f\"{col} {rule}\")\n",
        "    \n",
        "    if conditions:\n",
        "        condition_str = \" AND \".join(conditions)\n",
        "        return df.filter(condition_str)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all tables from configuration\n",
        "tables_config = bronze_config['tables']\n",
        "\n",
        "for table_name, table_config in tables_config.items():\n",
        "    print(f\"\\n\ud83d\udd04 Processing table: {table_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Read CSV file\n",
        "        source_file = table_config['source_file']\n",
        "        csv_path = f\"{RAW_DATA_PATH}/{source_file}\"\n",
        "        \n",
        "        print(f\"\ud83d\udcd6 Reading from: {csv_path}\")\n",
        "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
        "        \n",
        "        initial_count = df.count()\n",
        "        print(f\"\ud83d\udcca Initial row count: {initial_count}\")\n",
        "        \n",
        "        # Apply data quality rules\n",
        "        dq_rules = table_config.get('data_quality_rules', {})\n",
        "        \n",
        "        # Null checks\n",
        "        if 'null_checks' in dq_rules:\n",
        "            df = validate_null_checks(df, dq_rules['null_checks'])\n",
        "            print(f\"\u2705 Applied null checks: {dq_rules['null_checks']}\")\n",
        "        \n",
        "        # Duplicate checks\n",
        "        if 'duplicate_checks' in dq_rules:\n",
        "            df = validate_duplicate_checks(df, dq_rules['duplicate_checks'])\n",
        "            print(f\"\u2705 Applied duplicate checks: {dq_rules['duplicate_checks']}\")\n",
        "        \n",
        "        # Data type validation\n",
        "        if 'data_type_validation' in dq_rules:\n",
        "            df = validate_data_types(df, dq_rules['data_type_validation'])\n",
        "            print(f\"\u2705 Applied data type validation\")\n",
        "        \n",
        "        # Business rules\n",
        "        if 'business_rules' in dq_rules:\n",
        "            df = validate_business_rules(df, dq_rules['business_rules'])\n",
        "            print(f\"\u2705 Applied business rules\")\n",
        "        \n",
        "        final_count = df.count()\n",
        "        print(f\"\ud83d\udcca Final row count: {final_count}\")\n",
        "        \n",
        "        # Write to bronze layer\n",
        "        bronze_table_path = f\"{BRONZE_DATA_PATH}/{table_name}\"\n",
        "        df.write.mode(\"overwrite\").format(\"delta\").save(bronze_table_path)\n",
        "        print(f\"\ud83d\udcbe Written to bronze: {bronze_table_path}\")\n",
        "        \n",
        "        # Log success\n",
        "        run_id = log_audit(table_name, \"bronze\", \"SUCCESS\", final_count)\n",
        "        update_watermark(table_name, \"bronze\", None, now())\n",
        "        \n",
        "        print(f\"\u2705 {table_name} processed successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"\u274c Error processing {table_name}: {error_msg}\")\n",
        "        \n",
        "        # Log error\n",
        "        run_id = log_audit(table_name, \"bronze\", \"FAIL\", 0, error_msg)\n",
        "        log_error(run_id, table_name, \"bronze\", \"INGESTION\", error_msg, {})\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Bronze layer processing completed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}